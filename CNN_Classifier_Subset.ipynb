{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection and Classification of Thorax Diseases on NIH Chest Xray Dataset ( CNNs ) - Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = './Datasets/images-small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1000, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Emphysema</th>\n",
       "      <th>Fibrosis</th>\n",
       "      <th>Hernia</th>\n",
       "      <th>Infiltration</th>\n",
       "      <th>Mass</th>\n",
       "      <th>Nodule</th>\n",
       "      <th>PatientId</th>\n",
       "      <th>Pleural_Thickening</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008270_015.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00029855_001.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29855</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001297_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1297</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Image  Atelectasis  Cardiomegaly  Consolidation  Edema  \\\n",
       "0  00008270_015.png            0             0              0      0   \n",
       "1  00029855_001.png            1             0              0      0   \n",
       "2  00001297_000.png            0             0              0      0   \n",
       "\n",
       "   Effusion  Emphysema  Fibrosis  Hernia  Infiltration  Mass  Nodule  \\\n",
       "0         0          0         0       0             0     0       0   \n",
       "1         1          0         0       0             1     0       0   \n",
       "2         0          0         0       0             0     0       0   \n",
       "\n",
       "   PatientId  Pleural_Thickening  Pneumonia  Pneumothorax  \n",
       "0       8270                   0          0             0  \n",
       "1      29855                   0          0             0  \n",
       "2       1297                   1          0             0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = pd.read_csv('Datasets/train-subset.csv')\n",
    "print(f'Shape: {df_small.shape}')\n",
    "df_small.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 155.47it/s]\n"
     ]
    }
   ],
   "source": [
    "SIZE = 256\n",
    "X_dataset = []  \n",
    "for i in tqdm(range(df_small.shape[0])):\n",
    "    img = image.load_img(image_directory + df_small['Image'][i], target_size=(SIZE,SIZE,3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255.\n",
    "    X_dataset.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Images, Labels to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape :(1000, 256, 256, 3)\n",
      "Label Shape :(1000, 14)\n"
     ]
    }
   ],
   "source": [
    "X_dataset = np.array(X_dataset)\n",
    "# print(df['Image'][999])\n",
    "\n",
    "y_dataset = np.array(df.drop(['Image', 'PatientId'], axis=1))\n",
    "\n",
    "print(f'Image Shape :{X_dataset.shape}')\n",
    "print(f'Label Shape :{y_dataset.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape :((700, 256, 256, 3), (700, 14))\n",
      "Test Shape :((300, 256, 256, 3), (300, 14))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, random_state=20, test_size=0.3)\n",
    "\n",
    "print(f'Train Shape :{X_train.shape, y_train.shape}')\n",
    "print(f'Test Shape :{X_test.shape, y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 252, 252, 16)      1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 252, 252, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 126, 126, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 126, 126, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 122, 122, 32)      12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 61, 61, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 61, 61, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 61, 61, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 57, 57, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 1,357,422\n",
      "Trainable params: 1,357,070\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=\"relu\", input_shape=(SIZE,SIZE,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(14, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 25s 2s/step - loss: 0.8148 - accuracy: 0.0728 - val_loss: 0.6646 - val_accuracy: 0.0167\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 27s 2s/step - loss: 0.5092 - accuracy: 0.0640 - val_loss: 0.5109 - val_accuracy: 0.0133\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 26s 2s/step - loss: 0.4346 - accuracy: 0.0756 - val_loss: 0.3677 - val_accuracy: 0.0233\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 28s 3s/step - loss: 0.3721 - accuracy: 0.0999 - val_loss: 0.3454 - val_accuracy: 0.2900\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 27s 2s/step - loss: 0.3343 - accuracy: 0.1581 - val_loss: 0.2710 - val_accuracy: 0.0700\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 27s 2s/step - loss: 0.3004 - accuracy: 0.1125 - val_loss: 0.2182 - val_accuracy: 0.1433\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 27s 2s/step - loss: 0.2916 - accuracy: 0.0882 - val_loss: 0.2215 - val_accuracy: 0.1167\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 28s 3s/step - loss: 0.2710 - accuracy: 0.1271 - val_loss: 0.2366 - val_accuracy: 0.1033\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 29s 3s/step - loss: 0.2523 - accuracy: 0.1281 - val_loss: 0.2169 - val_accuracy: 0.1633\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 27s 2s/step - loss: 0.2426 - accuracy: 0.1304 - val_loss: 0.1890 - val_accuracy: 0.1433\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Loss & Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference | Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardiomegaly : Enlarged condition of Heart\n",
    "img = image.load_img('Datasets/00000001_000.png', target_size=(SIZE,SIZE,3))\n",
    "\n",
    "img = image.img_to_array(img)\n",
    "img = img/255.\n",
    "plt.imshow(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "classes = np.array(df.columns[2:]) #Get array of all classes\n",
    "proba = model.predict(img)  #Get probabilities for each class\n",
    "sorted_categories = np.argsort(proba[0])[:-11:-1]  #Get class names for top 10 categories\n",
    "\n",
    "#Print classes and corresponding probabilities\n",
    "for i in range(10):\n",
    "    print(\"{}\".format(classes[sorted_categories[i]])+\" ({:.3})\".format(proba[0][sorted_categories[i]]))\n",
    "\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy = \", (acc * 100.0), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
